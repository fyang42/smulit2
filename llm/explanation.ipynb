{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Explanation of the contents in the 'llm' folder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Contents:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. __init__.py\n",
    "2. llm.py\n",
    "3. core.py\n",
    "4. message.py\n",
    "5. messages.py\n",
    "6. groq.py\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. __init__.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code has 2 functions: It serves as an initializer for the packages and defines the public interface of the llm module."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. **Imports**: Here, we import the classes and functions from the various modules within the 'llm' package such as 'LLM', 'Groq', 'LLMType', 'Message', etc.\n",
    "2. **__all__**: This is the list which contains the list of all the names of the objects, in this case, the modules. This allows us to much control over the modules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# llm/__init__.py\n",
    "\n",
    "from llm.core import LLM\n",
    "from llm.groq import Groq\n",
    "from llm.llm import LLMType\n",
    "from llm.message import Message, Messages\n",
    "from llm.messages import transform_messages\n",
    "\n",
    "__all__ = [\n",
    "    \"LLM\",\n",
    "    \"Groq\",\n",
    "    \"LLMType\",\n",
    "    \"Message\",\n",
    "    \"Messages\",\n",
    "    \"transform_messages\",\n",
    "]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. llm/llm.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defines the 'LLMType' enumeration which repesents the various types of LLMs (in the event we want to use other LLMs like Llamas or Mistrals.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**BASE**, **INSTRUCT**, **NEITHER**: These values represent different types of LLMs that can be used, such as a base model, an instruction-tuned model, or neither."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# llm/llm.py\n",
    "\n",
    "from enum import Enum\n",
    "\n",
    "__all__ = [\"LLMType\"]\n",
    "\n",
    "\n",
    "class LLMType(Enum):\n",
    "    BASE = 0\n",
    "    INSTRUCT = 1\n",
    "    NEITHER = 2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. core.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we define the abstract base class (LLM), which is the foundation of the specific LLM implementations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**LLM Class**: \n",
    "1. Constructor which initializes the LLM with a name and optional system prompt and type.\n",
    "2. 'set_system_prompt' method: method to set or update system prompt\n",
    "3. Abstract methods - 'chat' and 'complete': abstract methods that must be used in any subclass. These define how the LLM handles chat and text completion tasks.\n",
    "4. 'generate' method: method to choose whether to use 'chat' or 'complete' methods based on the type of LLM. Allowing for easy switching and use between various types of LLMs.\n",
    "5. __call__: allows the LLM instance to be called as a function (calls the generate method)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# llm/core.py\n",
    "\n",
    "from abc import ABC, abstractmethod\n",
    "from typing import Optional, Union\n",
    "\n",
    "from llm.llm import LLMType\n",
    "from llm.message import Messages\n",
    "\n",
    "__all__ = [\"LLM\"]\n",
    "\n",
    "\n",
    "class LLM(ABC):\n",
    "    def __init__(self, name: str,\n",
    "                 system_prompt: str = \"\",\n",
    "                 type: Optional[Union[LLMType, int]] = LLMType.NEITHER):\n",
    "        self.name = name\n",
    "        self.system_prompt = system_prompt\n",
    "\n",
    "        if isinstance(type, LLMType):\n",
    "            self.type = type\n",
    "        elif isinstance(type, int) and 0 <= type <= 2:\n",
    "            self.type = LLMType(type)\n",
    "        elif isinstance(type, int):\n",
    "            raise ValueError(f\"Type {type} not recognized.\")\n",
    "        else:\n",
    "            raise TypeError(f\"Value {type} not of type 'LLMType' or 'int'\")\n",
    "\n",
    "    def set_system_prompt(self, system_prompt: str):\n",
    "        self.system_prompt = system_prompt\n",
    "        return self\n",
    "\n",
    "    @abstractmethod\n",
    "    def chat(self,\n",
    "             text: Messages,\n",
    "             max_new_tokens: int = 256,\n",
    "             temperature: float = 0.1) -> str:\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def complete(self,\n",
    "                 text: str,\n",
    "                 max_new_tokens: int = 256,\n",
    "                 temperature: float = 0.1) -> str:\n",
    "        pass\n",
    "\n",
    "    def generate(self,\n",
    "                 text: Messages,\n",
    "                 max_new_tokens: int = 256,\n",
    "                 temperature: float = 0.1,\n",
    "                 instruct: Optional[bool] = None) -> str:\n",
    "        type = None\n",
    "        if instruct is None:\n",
    "            if self.type == LLMType.BASE:\n",
    "                type = LLMType.BASE\n",
    "            else:\n",
    "                type = LLMType.INSTRUCT\n",
    "        elif instruct:\n",
    "            type = LLMType.INSTRUCT\n",
    "        else:\n",
    "            type = LLMType.BASE\n",
    "\n",
    "        if type == LLMType.INSTRUCT:\n",
    "            return self.chat(\n",
    "                text,\n",
    "                max_new_tokens=max_new_tokens,\n",
    "                temperature=temperature\n",
    "            )\n",
    "        else:\n",
    "            if not isinstance(text, str):\n",
    "                raise ValueError(\"Unsupported type for input 'text'\")\n",
    "            return self.complete(\n",
    "                text,\n",
    "                max_new_tokens=max_new_tokens,\n",
    "                temperature=temperature\n",
    "            )\n",
    "\n",
    "    def __call__(self,\n",
    "                 text: Messages,\n",
    "                 max_new_tokens: int = 256,\n",
    "                 temperature: float = 0.1,\n",
    "                 instruct: Optional[bool] = None) -> str:\n",
    "        return self.generate(text, max_new_tokens,\n",
    "                             temperature, instruct)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. llm/message.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defines the structure for chat interactions with the LLM."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Message class**: \n",
    "1. Attributes: defines a message with a role, such as user or system, and content.\n",
    "2. Message type: Union type which represents different possible formats for messages (list of 'Message' objects, list of dictionaries, or a simple string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# llm/message.py\n",
    "\n",
    "from typing import Union\n",
    "from pydantic import BaseModel\n",
    "\n",
    "__all__ = [\n",
    "    \"Message\",\n",
    "    \"Messages\"\n",
    "]\n",
    "\n",
    "\n",
    "class Message(BaseModel):\n",
    "    role: str\n",
    "    content: str\n",
    "\n",
    "\n",
    "Messages = Union[\n",
    "    list[Message],\n",
    "    list[dict[str, str]],\n",
    "    str\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. llm/messages.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "utility functions for working with messages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. **transform_messages** function:\n",
    "   - transforms input messages into a standard list of dictionaries format for compatibility with Groq API. \n",
    "   - checks format of input messages, handles various formats such as strings, list of 'Message' objects, list of dictionaries. \n",
    "   - Optionally, it inserts a system prompt at the beginning of the message list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# llm/messages.py\n",
    "\n",
    "from llm.message import Message, Messages\n",
    "\n",
    "__all__ = [\n",
    "    \"transform_messages\"\n",
    "]\n",
    "\n",
    "\n",
    "def transform_messages(input: Messages, \n",
    "                       system_prompt: str = \"\") -> list[dict[str, str]]:\n",
    "    messages: list[dict[str, str]]\n",
    "    if isinstance(input, str):\n",
    "        messages = [{\n",
    "            \"role\": \"user\",\n",
    "            \"content\": input\n",
    "        }]\n",
    "    elif isinstance(input, list) and isinstance(input[0], Message):\n",
    "        messages = [\n",
    "            dict(msg)\n",
    "            for msg in input\n",
    "        ]\n",
    "    elif isinstance(input, list) and isinstance(input[0], dict):\n",
    "        messages = input\n",
    "    \n",
    "    else:\n",
    "        raise TypeError(\"Unsupported format for messages item\")\n",
    "    \n",
    "    if messages[0][\"role\"] != \"system\" and system_prompt is not None and len(system_prompt.strip()) > 0:\n",
    "            messages.insert(0, {\"role\": \"system\", \"content\": system_prompt})\n",
    "    \n",
    "    return messages\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "llm/groq.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implementation of the LLM abstract class using the Groq API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 'Groq' class inherits the 'LLM' class and initializes the Groq client using an API key.\n",
    "- implements chat functionality using the Groq API which supports chat transformation and token limits\n",
    "- uses the 'chat' method for text completion by formatting input text in a specific way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# llm/groq.py\n",
    "\n",
    "import groq\n",
    "\n",
    "from typing import Optional, Union\n",
    "\n",
    "from llm.message import Messages\n",
    "from llm.llm import LLMType\n",
    "from llm.messages import transform_messages\n",
    "from llm.core import LLM\n",
    "\n",
    "__all__ = [\n",
    "    \"Groq\"\n",
    "]\n",
    "\n",
    "\n",
    "class Groq(LLM):\n",
    "    def __init__(self,\n",
    "                 model_id: str,\n",
    "                 api_key: str,\n",
    "                 system_prompt: str = \"\",\n",
    "                 type: Optional[Union[LLMType, int]] = LLMType.NEITHER):\n",
    "        super().__init__(\n",
    "            model_id, system_prompt,\n",
    "            type\n",
    "        )\n",
    "        self.client = groq.Groq(api_key=api_key)\n",
    "\n",
    "    def chat(self,\n",
    "             text: Messages,\n",
    "             max_new_tokens: int = 1024,\n",
    "             temperature: float = 0.1) -> str:\n",
    "        messages = transform_messages(text, self.system_prompt)\n",
    "\n",
    "        message = self.client.chat.completions.create(\n",
    "            max_tokens=max_new_tokens,\n",
    "            messages=messages,\n",
    "            temperature=temperature,\n",
    "            model=self.name\n",
    "        )\n",
    "        output = message.choices[0].message.content\n",
    "        return output\n",
    "\n",
    "    def complete(self,\n",
    "                 text: str,\n",
    "                 max_new_tokens: int = 1024,\n",
    "                 temperature: float = 0.1) -> str:\n",
    "        text = f\"Continue writing: {text}\"\n",
    "        \n",
    "        return self.chat(text, max_new_tokens=max_new_tokens, temperature=temperature)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
